{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using XGBoost\n",
    "\n",
    "*이 노트북은 [Amazon SageMaker Multi-Model Endpoints using XGBoost (영문 원본)](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_xgboost_home_value/xgboost_multi_model_endpoint_home_value.ipynb) 의 한국어 번역입니다.*\n",
    "\n",
    "고객들은 [Amazon SageMaker 멀티 모델 엔드포인트(multi-model endpoints)](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html)를 사용하여 최대 수천 개의 모델을 완벽하게 호스팅하는 엔드포인트를 생성할 수 있습니다. 이러한 엔드포인트는 공통 추론 컨테이너(common inference container)에서 제공할 수 있는 많은 모델 중 하나를 온디맨드(on demand)로 호출할 수 있어야 하고 자주 호출되지 않는 모델이 약간의 추가 대기 시간(latency) 허용이 가능한 사례들에 적합합니다. 지속적으로 낮은 추론 대기 시간이 필요한 애플리케이션의 경우 기존의 엔드포인트가 여전히 최선의 선택입니다.\n",
    "\n",
    "High level에서 Amazon SageMaker는 필요에 따라 멀티 모델 엔드포인트에 대한 모델 로딩 및 언로딩을 관리합니다. 특정 모델에 대한 호출 요청이 발생하면 Amazon SageMaker는 해당 모델에 할당된 인스턴스로 요청을 라우팅하고 S3에서 모델 아티팩트(model artifacts)를 해당 인스턴스로 다운로드한 다음 컨테이너의 메모리에 모델 로드를 시작합니다. 로딩이 완료되면 Amazon SageMaker는 요청된 호출을 수행하고 결과를 반환합니다. 모델이 선택된 인스턴스의 메모리에 이미 로드되어 있으면 다운로드 및 로딩 단계들을 건너 뛰고 즉시 호출이 수행됩니다.\n",
    "\n",
    "멀티 모델 엔드포인트 작성 및 사용 방법을 보여주기 위해, 이 노트북은 단일 위치의 주택 가격을 예측하는 XGBoost 모델을 사용하는 예시를 제공합니다. 이 도메인은 멀티 모델 엔드포인트를 쉽게 실험하기 위한 간단한 예제입니다.\n",
    "\n",
    "Amazon SageMaker 멀티 모델 엔드포인트 기능은 컨테이너를 가져 오는 프레임워크를 포함한 모든 머신 러닝 프레임워크 및 알고리즘에서 작동하도록 설계되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Build and register an XGBoost container that can serve multiple models](#Build-and-register-an-XGBoost-container-that-can-serve-multiple-models)\n",
    "1. [Generate synthetic data for housing models](#Generate-synthetic-data-for-housing-models)\n",
    "1. [Train multiple house value prediction models](#Train-multiple-house-value-prediction-models)\n",
    "1. [Import models into hosting](#Import-models-into-hosting)\n",
    "  1. [Deploy model artifacts to be found by the endpoint](#Deploy-model-artifacts-to-be-found-by-the-endpoint)\n",
    "  1. [Create the Amazon SageMaker model entity](#Create-the-Amazon-SageMaker-model-entity)\n",
    "  1. [Create the multi-model endpoint](#Create-the-multi-model-endpoint)\n",
    "1. [Exercise the multi-model endpoint](#Exercise-the-multi-model-endpoint)\n",
    "  1. [Dynamically deploy another model](#Dynamically-deploy-another-model)\n",
    "  1. [Invoke the newly deployed model](#Invoke-the-newly-deployed-model)\n",
    "  1. [Updating a model](#Updating-a-model)\n",
    "1. [Clean up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and register an XGBoost container that can serve multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론 컨테이너가 멀티 모델 엔드 포인트에서 여러 모델을 제공하려면 특정 모델의 로드(load), 나열(list), 가져오기(get), 언로드(unload) 및 호출(invoke)을 위한 [추가 API](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html)를 구현해야 합니다.\n",
    "\n",
    "[SageMaker XGBoost 컨테이너 저장소의 'mme' branch](https://github.com/aws/sagemaker-xgboost-container/tree/mme)는 멀티 모델 엔드포인트에 필요한 추가 컨테이너 API를 구현하는 HTTP 프론트엔드를 제공하는 프레임워크인 [Multi Model Server](https://github.com/awslabs/multi-model-server)를 사용하도록 SageMaker의 XGBoost 프레임워크 컨테이너를 조정하는 방법에 대한 예제 구현입니다. 또한 사용자 정의 프레임워크 (본 예시에서는 XGBoost 프레임워크)를 사용하여 모델을 제공하기 위한 플러그 가능한 백엔드 핸들러(pluggable backend handler)를 제공합니다.\n",
    "\n",
    "이 branch를 사용하여 모든 멀티 모델 엔드 포인트 컨테이너 요구 사항을 충족하는 XGBoost 컨테이너를 구축한 다음 해당 이미지를 Amazon Elastic Container Registry(ECR)에 업로드합니다. 이미지를 ECR에 업로드하면 새로운 ECR 저장소가 생성될 수 있으므로 이 노트북에는 일반 `SageMakerFullAccess` 권한 외에 권한이 필요합니다. 이러한 권한을 추가하는 가장 쉬운 방법은 관리형 정책 `AmazonEC2ContainerRegistryFullAccess`를 노트북 인스턴스를 시작하는 데 사용한 역할(role)에 추가하는 것입니다. 이 작업을 수행할 때 노트북 인스턴스를 다시 시작할 필요가 없으며 새 권한을 즉시 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_NAME = 'multi-model-xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "sha256:043284b5094346869c82aedf4248f70279624c8e79e469e06dc8a816bdd71ddf\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/hyperparameter_validation.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/__init__.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/metrics.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/exceptions.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/channel_validation.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/metadata.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "creating build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/__init__.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/training.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/checkpointing.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/encoder.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/data_utils.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/distributed.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/handler_service.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/serving.py -> build/lib/sagemaker_xgboost_container\n",
      "creating build/lib/sagemaker_xgboost_container/metrics\n",
      "copying src/sagemaker_xgboost_container/metrics/__init__.py -> build/lib/sagemaker_xgboost_container/metrics\n",
      "copying src/sagemaker_xgboost_container/metrics/custom_metrics.py -> build/lib/sagemaker_xgboost_container/metrics\n",
      "creating build/lib/sagemaker_xgboost_container/constants\n",
      "copying src/sagemaker_xgboost_container/constants/xgb_content_types.py -> build/lib/sagemaker_xgboost_container/constants\n",
      "copying src/sagemaker_xgboost_container/constants/__init__.py -> build/lib/sagemaker_xgboost_container/constants\n",
      "copying src/sagemaker_xgboost_container/constants/xgb_constants.py -> build/lib/sagemaker_xgboost_container/constants\n",
      "copying src/sagemaker_xgboost_container/constants/sm_env_constants.py -> build/lib/sagemaker_xgboost_container/constants\n",
      "creating build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/hyperparameter_validation.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/__init__.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/train.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/handler_service.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/metrics.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/integration.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/channel_validation.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/inference_errors.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/train_utils.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/metadata.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "creating build/lib/sagemaker_xgboost_container/dmlc_patch\n",
      "copying src/sagemaker_xgboost_container/dmlc_patch/__init__.py -> build/lib/sagemaker_xgboost_container/dmlc_patch\n",
      "copying src/sagemaker_xgboost_container/dmlc_patch/tracker.py -> build/lib/sagemaker_xgboost_container/dmlc_patch\n",
      "creating build/lib/sagemaker_xgboost_container/mms_patch\n",
      "copying src/sagemaker_xgboost_container/mms_patch/__init__.py -> build/lib/sagemaker_xgboost_container/mms_patch\n",
      "copying src/sagemaker_xgboost_container/mms_patch/mms_transformer.py -> build/lib/sagemaker_xgboost_container/mms_patch\n",
      "copying src/sagemaker_xgboost_container/mms_patch/model_server.py -> build/lib/sagemaker_xgboost_container/mms_patch\n",
      "installing to build/bdist.linux-x86_64/wheel\n",
      "running install\n",
      "running install_lib\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/wheel\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/hyperparameter_validation.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/metrics.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/exceptions.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/channel_validation.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/metadata.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/metrics\n",
      "copying build/lib/sagemaker_xgboost_container/metrics/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/metrics\n",
      "copying build/lib/sagemaker_xgboost_container/metrics/custom_metrics.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/metrics\n",
      "copying build/lib/sagemaker_xgboost_container/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/constants/xgb_content_types.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/constants/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/constants/xgb_constants.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/constants/sm_env_constants.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/training.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/checkpointing.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/encoder.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/data_utils.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/distributed.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/hyperparameter_validation.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/train.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/handler_service.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/metrics.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/integration.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/channel_validation.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/inference_errors.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/train_utils.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/metadata.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/handler_service.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/serving.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/dmlc_patch\n",
      "copying build/lib/sagemaker_xgboost_container/dmlc_patch/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/dmlc_patch\n",
      "copying build/lib/sagemaker_xgboost_container/dmlc_patch/tracker.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/dmlc_patch\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/mms_patch\n",
      "copying build/lib/sagemaker_xgboost_container/mms_patch/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/mms_patch\n",
      "copying build/lib/sagemaker_xgboost_container/mms_patch/mms_transformer.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/mms_patch\n",
      "copying build/lib/sagemaker_xgboost_container/mms_patch/model_server.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/mms_patch\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating src/sagemaker_xgboost_container.egg-info\n",
      "writing src/sagemaker_xgboost_container.egg-info/PKG-INFO\n",
      "writing dependency_links to src/sagemaker_xgboost_container.egg-info/dependency_links.txt\n",
      "writing entry points to src/sagemaker_xgboost_container.egg-info/entry_points.txt\n",
      "writing requirements to src/sagemaker_xgboost_container.egg-info/requires.txt\n",
      "writing top-level names to src/sagemaker_xgboost_container.egg-info/top_level.txt\n",
      "writing manifest file 'src/sagemaker_xgboost_container.egg-info/SOURCES.txt'\n",
      "reading manifest file 'src/sagemaker_xgboost_container.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'src/sagemaker_xgboost_container.egg-info/SOURCES.txt'\n",
      "Copying src/sagemaker_xgboost_container.egg-info to build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container-1.0-py3.6.egg-info\n",
      "running install_scripts\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container-1.0.dist-info/WHEEL\n",
      "creating '/home/ec2-user/SageMaker/sagemaker-xgboost-container/dist/sagemaker_xgboost_container-1.0-py2.py3-none-any.whl' and adding '.' to it\n",
      "adding 'sagemaker_algorithm_toolkit/__init__.py'\n",
      "adding 'sagemaker_algorithm_toolkit/channel_validation.py'\n",
      "adding 'sagemaker_algorithm_toolkit/exceptions.py'\n",
      "adding 'sagemaker_algorithm_toolkit/hyperparameter_validation.py'\n",
      "adding 'sagemaker_algorithm_toolkit/metadata.py'\n",
      "adding 'sagemaker_algorithm_toolkit/metrics.py'\n",
      "adding 'sagemaker_xgboost_container/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/checkpointing.py'\n",
      "adding 'sagemaker_xgboost_container/data_utils.py'\n",
      "adding 'sagemaker_xgboost_container/distributed.py'\n",
      "adding 'sagemaker_xgboost_container/encoder.py'\n",
      "adding 'sagemaker_xgboost_container/handler_service.py'\n",
      "adding 'sagemaker_xgboost_container/serving.py'\n",
      "adding 'sagemaker_xgboost_container/training.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/channel_validation.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/handler_service.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/hyperparameter_validation.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/inference_errors.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/integration.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/metadata.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/metrics.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/train.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/train_utils.py'\n",
      "adding 'sagemaker_xgboost_container/constants/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/constants/sm_env_constants.py'\n",
      "adding 'sagemaker_xgboost_container/constants/xgb_constants.py'\n",
      "adding 'sagemaker_xgboost_container/constants/xgb_content_types.py'\n",
      "adding 'sagemaker_xgboost_container/dmlc_patch/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/dmlc_patch/tracker.py'\n",
      "adding 'sagemaker_xgboost_container/metrics/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/metrics/custom_metrics.py'\n",
      "adding 'sagemaker_xgboost_container/mms_patch/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/mms_patch/mms_transformer.py'\n",
      "adding 'sagemaker_xgboost_container/mms_patch/model_server.py'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/entry_points.txt'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/top_level.txt'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/WHEEL'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/METADATA'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/RECORD'\n",
      "removing build/bdist.linux-x86_64/wheel\n",
      "sha256:7fb2cb2d61b48cd28e028fc03d5ddc5b4e5e743cd1ed0e6755b996ea243a8ff1\n",
      "The push refers to repository [143656149352.dkr.ecr.ap-northeast-2.amazonaws.com/multi-model-xgboost]\n",
      "4cf50bcb288a: Preparing\n",
      "88b3adea9017: Preparing\n",
      "9556539b6411: Preparing\n",
      "f228ceb39ab2: Preparing\n",
      "3259e4859d21: Preparing\n",
      "3ab7a974a169: Preparing\n",
      "00d0c1378636: Preparing\n",
      "547dffa71418: Preparing\n",
      "6463b936a4b4: Preparing\n",
      "860190f5fcc3: Preparing\n",
      "52578e5a63f6: Preparing\n",
      "c6d97ea622cc: Preparing\n",
      "d3ccd770911d: Preparing\n",
      "070a8876e05f: Preparing\n",
      "611f6c285753: Preparing\n",
      "a2c7f3b9527f: Preparing\n",
      "777d8261538c: Preparing\n",
      "479f1f2cc1c7: Preparing\n",
      "774f773f965c: Preparing\n",
      "6821959532ed: Preparing\n",
      "5571d219faca: Preparing\n",
      "b7dddd9c9825: Preparing\n",
      "c31d110fd136: Preparing\n",
      "aa7f8c8d5f39: Preparing\n",
      "48817fbd6c92: Preparing\n",
      "1b039d138968: Preparing\n",
      "7082d7d696f8: Preparing\n",
      "a2c7f3b9527f: Waiting\n",
      "3ab7a974a169: Waiting\n",
      "777d8261538c: Waiting\n",
      "479f1f2cc1c7: Waiting\n",
      "00d0c1378636: Waiting\n",
      "774f773f965c: Waiting\n",
      "547dffa71418: Waiting\n",
      "6821959532ed: Waiting\n",
      "6463b936a4b4: Waiting\n",
      "860190f5fcc3: Waiting\n",
      "5571d219faca: Waiting\n",
      "52578e5a63f6: Waiting\n",
      "b7dddd9c9825: Waiting\n",
      "c6d97ea622cc: Waiting\n",
      "c31d110fd136: Waiting\n",
      "d3ccd770911d: Waiting\n",
      "aa7f8c8d5f39: Waiting\n",
      "070a8876e05f: Waiting\n",
      "48817fbd6c92: Waiting\n",
      "611f6c285753: Waiting\n",
      "1b039d138968: Waiting\n",
      "7082d7d696f8: Waiting\n",
      "9556539b6411: Pushed\n",
      "88b3adea9017: Pushed\n",
      "4cf50bcb288a: Pushed\n",
      "f228ceb39ab2: Pushed\n",
      "3259e4859d21: Pushed\n",
      "00d0c1378636: Pushed\n",
      "3ab7a974a169: Pushed\n",
      "6463b936a4b4: Pushed\n",
      "547dffa71418: Pushed\n",
      "860190f5fcc3: Pushed\n",
      "070a8876e05f: Pushed\n",
      "52578e5a63f6: Pushed\n",
      "c6d97ea622cc: Pushed\n",
      "a2c7f3b9527f: Pushed\n",
      "479f1f2cc1c7: Pushed\n",
      "774f773f965c: Pushed\n",
      "5571d219faca: Pushed\n",
      "b7dddd9c9825: Pushed\n",
      "d3ccd770911d: Pushed\n",
      "aa7f8c8d5f39: Pushed\n",
      "48817fbd6c92: Pushed\n",
      "1b039d138968: Pushed\n",
      "611f6c285753: Pushed\n",
      "7082d7d696f8: Pushed\n",
      "6821959532ed: Pushed\n",
      "c31d110fd136: Pushed\n",
      "777d8261538c: Pushed\n",
      "latest: digest: sha256:7e18402080dbba46e34f8206f1255f4121ccdfdbbb4a1db7f09fdcbccafe784e size: 5969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Cloning into 'sagemaker-xgboost-container'...\n"
     ]
    }
   ],
   "source": [
    "%%sh -s $ALGORITHM_NAME\n",
    "\n",
    "algorithm_name=$1\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration\n",
    "region=$(aws configure get region)\n",
    "\n",
    "ecr_image=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email --registry-ids ${account})\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# First clear out any prior version of the cloned repo\n",
    "rm -rf sagemaker-xgboost-container/\n",
    "\n",
    "# Clone the xgboost container repo\n",
    "git clone --single-branch --branch mme https://github.com/aws/sagemaker-xgboost-container.git\n",
    "cd sagemaker-xgboost-container/\n",
    "\n",
    "# Build the \"base\" container image that encompasses the installation of the\n",
    "# XGBoost framework and all of the dependencies needed.\n",
    "docker build -q -t xgboost-container-base:0.90-2-cpu-py3 -f docker/0.90-2/base/Dockerfile.cpu .\n",
    "\n",
    "# Create the SageMaker XGBoost Container Python package.\n",
    "python setup.py bdist_wheel --universal\n",
    "\n",
    "# Build the \"final\" container image that encompasses the installation of the\n",
    "# code that implements the SageMaker multi-model container requirements.\n",
    "docker build -q -t ${algorithm_name} -f docker/0.90-2/final/Dockerfile.cpu .\n",
    "\n",
    "docker tag ${algorithm_name} ${ecr_image}\n",
    "\n",
    "docker push ${ecr_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data for housing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "PARALLEL_TRAINING_JOBS = 4 # len(LOCATIONS) if your account limits can handle it\n",
    "MAX_YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_price(house):\n",
    "    _base_price = int(house['SQUARE_FEET'] * 150)\n",
    "    _price = int(_base_price + (10000 * house['NUM_BEDROOMS']) + \\\n",
    "                               (15000 * house['NUM_BATHROOMS']) + \\\n",
    "                               (15000 * house['LOT_ACRES']) + \\\n",
    "                               (15000 * house['GARAGE_SPACES']) - \\\n",
    "                               (5000 * (MAX_YEAR - house['YEAR_BUILT'])))\n",
    "    return _price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_house():\n",
    "    _house = {'SQUARE_FEET':   int(np.random.normal(3000, 750)),\n",
    "              'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "              'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "              'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "              'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "              'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10)))}\n",
    "    _price = gen_price(_house)\n",
    "    return [_price, _house['YEAR_BUILT'],   _house['SQUARE_FEET'], \n",
    "                    _house['NUM_BEDROOMS'], _house['NUM_BATHROOMS'], \n",
    "                    _house['LOT_ACRES'],    _house['GARAGE_SPACES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_houses(num_houses):\n",
    "    _house_list = []\n",
    "    for i in range(num_houses):\n",
    "        _house_list.append(gen_random_house())\n",
    "    _df = pd.DataFrame(_house_list, \n",
    "                       columns=['PRICE',        'YEAR_BUILT',    'SQUARE_FEET',  'NUM_BEDROOMS',\n",
    "                                'NUM_BATHROOMS','LOT_ACRES',     'GARAGE_SPACES'])\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple house value prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION     = boto3.Session().region_name\n",
    "BUCKET     = sagemaker_session.default_bucket()\n",
    "\n",
    "MULTI_MODEL_XGBOOST_IMAGE = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(ACCOUNT_ID, REGION, \n",
    "                                                                           ALGORITHM_NAME)\n",
    "\n",
    "DATA_PREFIX            = 'DEMO_MME_XGBOOST'\n",
    "HOUSING_MODEL_NAME     = 'housing'\n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "\n",
    "TRAIN_INSTANCE_TYPE    = 'ml.m4.xlarge'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m4.xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split a given dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 7\n",
    "SPLIT_RATIOS = [0.6, 0.3, 0.1]\n",
    "\n",
    "def split_data(df):\n",
    "    # split data into train and test sets\n",
    "    seed      = SEED\n",
    "    val_size  = SPLIT_RATIOS[1]\n",
    "    test_size = SPLIT_RATIOS[2]\n",
    "    \n",
    "    num_samples = df.shape[0]\n",
    "    X1 = df.values[:num_samples, 1:] # keep only the features, skip the target, all rows\n",
    "    Y1 = df.values[:num_samples, :1] # keep only the target, all rows\n",
    "\n",
    "    # Use split ratios to divide up into train/val/test\n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "        train_test_split(X1, Y1, test_size=(test_size + val_size), random_state=seed)\n",
    "    # Of the remaining non-training samples, give proper ratio to validation and to test\n",
    "    X_test, X_test, y_test, y_test = \\\n",
    "        train_test_split(X_val, y_val, test_size=(test_size / (test_size + val_size)), \n",
    "                         random_state=seed)\n",
    "    # reassemble the datasets with target in first column and features after that\n",
    "    _train = np.concatenate([y_train, X_train], axis=1)\n",
    "    _val   = np.concatenate([y_val,   X_val],   axis=1)\n",
    "    _test  = np.concatenate([y_test,  X_test],  axis=1)\n",
    "\n",
    "    return _train, _val, _test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch a single training job for a given housing location\n",
    "\n",
    "모델 학습 시, 기존 SageMaker 모델과 동일한 방식으로 학습하기 때문에 멀티 모델 엔트 포인트에 특화된 기능을 따로 구현하실 필요가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_training_job(location):\n",
    "    # clear out old versions of the data\n",
    "    s3_bucket = s3.Bucket(BUCKET)\n",
    "    full_input_prefix = '{}/model_prep/{}'.format(DATA_PREFIX, location)\n",
    "    s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "    local_folder = 'data/{}'.format(location)\n",
    "    inputs = sagemaker_session.upload_data(path=local_folder, key_prefix=full_input_prefix)\n",
    "    print('Training data uploaded: {}'.format(inputs))\n",
    "    \n",
    "    _job = 'xgb-{}'.format(location.replace('_', '-'))\n",
    "    full_output_prefix = '{}/model_artifacts/{}'.format(DATA_PREFIX, location)\n",
    "    s3_output_path = 's3://{}/{}'.format(BUCKET, full_output_prefix)\n",
    "\n",
    "    xgb = sagemaker.estimator.Estimator(MULTI_MODEL_XGBOOST_IMAGE, role, \n",
    "                                        train_instance_count=1, train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "                                        output_path=s3_output_path, base_job_name=_job,\n",
    "                                        sagemaker_session=sagemaker_session)\n",
    "    xgb.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, silent=0, \n",
    "                            early_stopping_rounds=5, objective='reg:linear', num_round=25) \n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    train_input = sagemaker.s3_input(s3_data=inputs+'/train', \n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    val_input   = sagemaker.s3_input(s3_data=inputs+'/val', \n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "\n",
    "    xgb.fit(remote_inputs, wait=False)\n",
    "    \n",
    "    return xgb.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kick off a model training job for each housing location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_locally(location, train, val, test):\n",
    "    os.makedirs('data/{}/train'.format(location))\n",
    "    np.savetxt( 'data/{0}/train/{0}_train.csv'.format(location), train, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs('data/{}/val'.format(location))\n",
    "    np.savetxt( 'data/{0}/val/{0}_val.csv'.format(location),     val, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs('data/{}/test'.format(location))\n",
    "    np.savetxt( 'data/{0}/test/{0}_test.csv'.format(location),   test, delimiter=',', fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_training_job(location):\n",
    "    # clear out old versions of the data\n",
    "    s3_bucket = s3.Bucket(BUCKET)\n",
    "    full_input_prefix = '{}/model_prep/{}'.format(DATA_PREFIX, location)\n",
    "    s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "    local_folder = 'data/{}'.format(location)\n",
    "    inputs = sagemaker_session.upload_data(path=local_folder, key_prefix=full_input_prefix)\n",
    "    print('Training data uploaded: {}'.format(inputs))\n",
    "    \n",
    "    _job = 'xgb-{}'.format(location.replace('_', '-'))\n",
    "    full_output_prefix = '{}/model_artifacts/{}'.format(DATA_PREFIX, location)\n",
    "    s3_output_path = 's3://{}/{}'.format(BUCKET, full_output_prefix)\n",
    "\n",
    "    xgb = sagemaker.estimator.Estimator(MULTI_MODEL_XGBOOST_IMAGE, role, \n",
    "                                        train_instance_count=1, train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "                                        output_path=s3_output_path, base_job_name=_job,\n",
    "                                        sagemaker_session=sagemaker_session)\n",
    "    xgb.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, silent=0, \n",
    "                            early_stopping_rounds=5, objective='reg:linear', num_round=25) \n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    train_input = sagemaker.s3_input(s3_data=inputs+'/train', \n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    val_input   = sagemaker.s3_input(s3_data=inputs+'/val', \n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "\n",
    "    xgb.fit(remote_inputs, wait=False)\n",
    "    \n",
    "    return xgb.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded: s3://sagemaker-ap-northeast-2-143656149352/DEMO_MME_XGBOOST/model_prep/NewYork_NY\n",
      "Training data uploaded: s3://sagemaker-ap-northeast-2-143656149352/DEMO_MME_XGBOOST/model_prep/LosAngeles_CA\n",
      "Training data uploaded: s3://sagemaker-ap-northeast-2-143656149352/DEMO_MME_XGBOOST/model_prep/Chicago_IL\n",
      "Training data uploaded: s3://sagemaker-ap-northeast-2-143656149352/DEMO_MME_XGBOOST/model_prep/Houston_TX\n",
      "4 training jobs launched: ['xgb-NewYork-NY-2019-11-27-06-15-54-555', 'xgb-LosAngeles-CA-2019-11-27-06-15-55-036', 'xgb-Chicago-IL-2019-11-27-06-15-56-286', 'xgb-Houston-TX-2019-11-27-06-15-57-402']\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "training_jobs = []\n",
    "\n",
    "shutil.rmtree('data', ignore_errors=True)\n",
    "\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    _houses = gen_houses(NUM_HOUSES_PER_LOCATION)\n",
    "    _train, _val, _test = split_data(_houses)\n",
    "    save_data_locally(loc, _train, _val, _test)\n",
    "    _job = launch_training_job(loc)\n",
    "    training_jobs.append(_job)\n",
    "print('{} training jobs launched: {}'.format(len(training_jobs), training_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for all model training to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_training_job_to_complete(job_name):\n",
    "    print('Waiting for job {} to complete...'.format(job_name))\n",
    "    resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    status = resp['TrainingJobStatus']\n",
    "    while status=='InProgress':\n",
    "        time.sleep(60)\n",
    "        resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "        status = resp['TrainingJobStatus']\n",
    "        if status == 'InProgress':\n",
    "            print('{} job status: {}'.format(job_name, status))\n",
    "    print('DONE. Status for {} is {}\\n'.format(job_name, status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job xgb-NewYork-NY-2019-11-27-06-15-54-555 to complete...\n",
      "DONE. Status for xgb-NewYork-NY-2019-11-27-06-15-54-555 is Completed\n",
      "\n",
      "Waiting for job xgb-LosAngeles-CA-2019-11-27-06-15-55-036 to complete...\n",
      "DONE. Status for xgb-LosAngeles-CA-2019-11-27-06-15-55-036 is Completed\n",
      "\n",
      "Waiting for job xgb-Chicago-IL-2019-11-27-06-15-56-286 to complete...\n",
      "DONE. Status for xgb-Chicago-IL-2019-11-27-06-15-56-286 is Completed\n",
      "\n",
      "Waiting for job xgb-Houston-TX-2019-11-27-06-15-57-402 to complete...\n",
      "DONE. Status for xgb-Houston-TX-2019-11-27-06-15-57-402 is Completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wait for the jobs to finish\n",
    "for j in training_jobs:\n",
    "    wait_for_training_job_to_complete(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import models into hosting\n",
    "멀티 모델 엔드포인트의 가장 큰 차이점은 모델 엔티티(Model entity)를 작성할 때 컨테이너의 `MultiModel`은 엔드포인트에서 호출할 수 있는 모델 아티팩트가 있는 S3 접두부(prefix)입니다. 나머지 S3 경로는 실제로 모델을 호출할 때 지정됩니다. 슬래시로 위치를 닫아야 하는 점을 기억해 주세요.\n",
    "\n",
    "컨테이너의 `Mode`는 컨테이너가 여러 모델을 호스팅함을 나타내기 위해 `MultiModel`로 지정됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model artifacts to be found by the endpoint\n",
    "상술한 바와 같이, 멀티 모델 엔드 포인트는 S3의 특정 위치에서 모델 아티팩트를 찾도록 구성됩니다. 학습된 각 모델에 대해 모델 아티팩트를 해당 위치에 복사합니다.\n",
    "\n",
    "이 예에서는 모든 모델들을 단일 폴더에 저장합니다. 멀티 모델 엔드 포인트의 구현은 임의의 폴더 구조를 허용할 만큼 유연합니다. 예를 들어 일련의 하우징 모델의 경우 각 지역마다 최상위 폴더가 있을 수 있으며 모델 아티팩트는 해당 지역 폴더로 복사됩니다. 이러한 모델을 호출할 때 참조되는 대상 모델에는 폴더 경로가 포함됩니다. 예를 들어 `northeast/Boston_MA.tar.gz`입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_model_artifacts(model_data_url):\n",
    "    # extract the s3 key from the full url to the model artifacts\n",
    "    _s3_key = model_data_url.split('s3://{}/'.format(BUCKET))[1]\n",
    "    # get the part of the key that identifies the model within the model artifacts folder\n",
    "    _model_name_plus = _s3_key[_s3_key.find('model_artifacts') + len('model_artifacts') + 1:]\n",
    "    # finally, get the unique model name (e.g., \"NewYork_NY\")\n",
    "    _model_name = re.findall('^(.*?)/', _model_name_plus)[0]\n",
    "    return _s3_key, _model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the model artifacts from the original output of the training job to the place in\n",
    "# s3 where the multi model endpoint will dynamically load individual models\n",
    "def deploy_artifacts_to_mme(job_name):\n",
    "    _resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    _source_s3_key, _model_name = parse_model_artifacts(_resp['ModelArtifacts']['S3ModelArtifacts'])\n",
    "    _copy_source = {'Bucket': BUCKET, 'Key': _source_s3_key}\n",
    "    _key = '{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, _model_name)\n",
    "    \n",
    "    print('Copying {} model\\n   from: {}\\n     to: {}...'.format(_model_name, _source_s3_key, _key))\n",
    "    s3_client.copy_object(Bucket=BUCKET, CopySource=_copy_source, Key=_key)\n",
    "    return _key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*의도적으로 첫 번째 모델을 복사하지 않는다는 점을 유의해 주세요.*. 첫 번째 모델은 향후 실습 과정에서 복사하여 이미 실행 중인 엔드포인트에 새 모델을 동적으로 추가하는 방법을 보여주기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old model artifacts from DEMO_MME_XGBOOST/multi_model_artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, clear out old versions of the model artifacts from previous runs of this notebook\n",
    "s3 = boto3.resource('s3')\n",
    "s3_bucket = s3.Bucket(BUCKET)\n",
    "full_input_prefix = '{}/multi_model_artifacts'.format(DATA_PREFIX)\n",
    "print('Removing old model artifacts from {}'.format(full_input_prefix))\n",
    "s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'xgb-LosAngeles-CA-2019-11-27-06-15-55-036'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# make a copy of the model artifacts from the original output of the training job to the place in\n",
    "# s3 where the multi model endpoint will dynamically load individual models\n",
    "def deploy_artifacts_to_mme(job_name):\n",
    "    _resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    #set_trace() #this one triggers the debugger    \n",
    "    _source_s3_key, _model_name = parse_model_artifacts(_resp['ModelArtifacts']['S3ModelArtifacts'])\n",
    "    \n",
    "    #'DEMO_MME_XGBOOST/model_artifacts/LosAngeles_CA/xgb-LosAngeles-CA-2019-11-27-06-15-55-036/output/model.tar.gz'\n",
    "    # model_name = 'LosAngeles_CA'\n",
    "    _copy_source = {'Bucket': BUCKET, 'Key': _source_s3_key}\n",
    "    _key = '{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, _model_name)\n",
    "    \n",
    "    print('Copying {} model\\n   from: {}\\n     to: {}...'.format(_model_name, _source_s3_key, _key))\n",
    "    s3_client.copy_object(Bucket=BUCKET, CopySource=_copy_source, Key=_key)\n",
    "    return _key\n",
    "\n",
    "\n",
    "def parse_model_artifacts(model_data_url):\n",
    "    # extract the s3 key from the full url to the model artifacts\n",
    "    _s3_key = model_data_url.split('s3://{}/'.format(BUCKET))[1]\n",
    "    # get the part of the key that identifies the model within the model artifacts folder\n",
    "    _model_name_plus = _s3_key[_s3_key.find('model_artifacts') + len('model_artifacts') + 1:]\n",
    "    # finally, get the unique model name (e.g., \"NewYork_NY\")\n",
    "    _model_name = re.findall('^(.*?)/', _model_name_plus)[0]\n",
    "    return _s3_key, _model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying LosAngeles_CA model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/LosAngeles_CA/xgb-LosAngeles-CA-2019-11-27-06-15-55-036/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/LosAngeles_CA.tar.gz...\n",
      "Copying Chicago_IL model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Chicago_IL/xgb-Chicago-IL-2019-11-27-06-15-56-286/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Chicago_IL.tar.gz...\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Houston_TX/xgb-Houston-TX-2019-11-27-06-15-57-402/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Houston_TX.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "# copy every model except the first one\n",
    "for job in training_jobs[1:]:\n",
    "    deploy_artifacts_to_mme(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Amazon SageMaker model entity\n",
    "`boto3`을 사용하여 모델 엔터티를 만듭니다. 단일 모델을 설명하는 대신 멀티 모델 시맨틱(semantics)의 사용을 나타내며 모든 특정 모델 아티팩트의 소스 위치를 식별합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_model_entity(multi_model_name, role):\n",
    "    # establish the place in S3 from which the endpoint will pull individual models\n",
    "    _model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "    print(_model_url)\n",
    "    _container = {\n",
    "        'Image':        MULTI_MODEL_XGBOOST_IMAGE,\n",
    "        'ModelDataUrl': _model_url,\n",
    "        'Mode':         'MultiModel'\n",
    "    }\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName = multi_model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        Containers = [_container])\n",
    "    \n",
    "    return _model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-2-143656149352/DEMO_MME_XGBOOST/multi_model_artifacts/\n",
      "Multi model name: housing-2019-11-27-06-47-21\n"
     ]
    }
   ],
   "source": [
    "multi_model_name = '{}-{}'.format(HOUSING_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "model_url = create_multi_model_entity(multi_model_name, role)\n",
    "print('Multi model name: {}'.format(multi_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the multi-model endpoint\n",
    "\n",
    "멀티 모델 엔드포인트에 대한 SageMaker 엔드포인트 설정(config)에는 특별한 것이 없습니다. 예상 예측 워크로드에 적합한 인스턴스 유형과 인스턴스 수를 고려해야 합니다. 개별 모델의 수와 크기에 따라 메모리 요구 사항이 변동합니다.\n",
    "\n",
    "엔드포인트 설정이 완료되면 엔드포인트 생성(creation)은 간단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: housing-2019-11-27-06-47-21\n",
      "Endpoint name: housing-2019-11-27-06-47-21\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = multi_model_name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': ENDPOINT_INSTANCE_TYPE,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': multi_model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = multi_model_name\n",
    "print('Endpoint name: ' + endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:ap-northeast-2:143656149352:endpoint/housing-2019-11-27-06-47-21\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for housing-2019-11-27-06-47-21 endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise the multi-model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke multiple individual models hosted behind a single endpoint\n",
    "\n",
    "여기서 여러분은 특정 위치 기반 주택 모델을 무작위로 선택하는 것을 반복합니다. 주어진 모델의 첫번째 호출에 대해 지불된 콜드 스타트(cold start) 비용이 과금된다는 점을 알아 두세요. 동일한 모델의 후속 호출은 이미 메모리에 로드된 모델을 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_house_value(features, model_name):\n",
    "    print('Using model {} to predict price of this house: {}'.format(full_model_name,\n",
    "                                                                     features))\n",
    "    body = ','.join(map(str, features)) + '\\n'\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "                        EndpointName=endpoint_name,\n",
    "                        ContentType='text/csv',\n",
    "                        TargetModel=full_model_name,\n",
    "                        Body=body)\n",
    "    predicted_value = json.loads(response['Body'].read())[0]\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value, int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the models that the endpoint has at its disposal:\n",
      "2019-11-27 06:46:19   11.0 KiB Chicago_IL.tar.gz\n",
      "2019-11-27 06:46:19   10.9 KiB Houston_TX.tar.gz\n",
      "2019-11-27 06:46:19   10.5 KiB LosAngeles_CA.tar.gz\n",
      "\n",
      "Total Objects: 3\n",
      "   Total Size: 32.4 KiB\n"
     ]
    }
   ],
   "source": [
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls --human-readable --summarize $model_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 모델에 대한 첫번째 요청을 완료하는 데 걸리는 시간은 S3에서 모델을 다운로드하여 메모리에 로드하기 위한 추가 대기 시간(콜드 스타트)이 필요합니다. 후속 호출은 모델이 이미 로드되었으므로 추가 오버헤드없이 완료됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Houston_TX\n",
      "Using model Houston_TX.tar.gz to predict price of this house: [1982, 2682, 6, 2.5, 0.89, 2]\n",
      "$347,410.53, took 1,450 ms\n",
      "\n",
      "Chicago_IL\n",
      "Using model Chicago_IL.tar.gz to predict price of this house: [1983, 2635, 4, 1.0, 0.95, 0]\n",
      "$312,624.59, took 909 ms\n",
      "\n",
      "Houston_TX\n",
      "Using model Houston_TX.tar.gz to predict price of this house: [1964, 2775, 2, 3.0, 1.29, 0]\n",
      "$290,677.59, took 17 ms\n",
      "\n",
      "Chicago_IL\n",
      "Using model Chicago_IL.tar.gz to predict price of this house: [1995, 2551, 5, 2.5, 1.53, 3]\n",
      "$412,950.59, took 16 ms\n",
      "\n",
      "Houston_TX\n",
      "Using model Houston_TX.tar.gz to predict price of this house: [1988, 2117, 2, 2.5, 1.07, 1]\n",
      "$275,964.59, took 18 ms\n",
      "\n",
      "LosAngeles_CA\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [2012, 4253, 3, 2.0, 0.86, 1]\n",
      "$659,443.25, took 894 ms\n",
      "\n",
      "Houston_TX\n",
      "Using model Houston_TX.tar.gz to predict price of this house: [1988, 2759, 2, 2.5, 0.42, 2]\n",
      "$346,578.16, took 21 ms\n",
      "\n",
      "LosAngeles_CA\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [1980, 2940, 2, 2.5, 1.7, 2]\n",
      "$345,930.94, took 16 ms\n",
      "\n",
      "LosAngeles_CA\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [2007, 3044, 6, 2.5, 0.97, 1]\n",
      "$527,157.00, took 15 ms\n",
      "\n",
      "Houston_TX\n",
      "Using model Houston_TX.tar.gz to predict price of this house: [1995, 2805, 6, 1.0, 0.89, 2]\n",
      "$421,496.19, took 13 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate through invocations with random inputs against a random model showing results and latency\n",
    "for i in range(10):\n",
    "    model_name = LOCATIONS[np.random.randint(1, len(LOCATIONS[:PARALLEL_TRAINING_JOBS]))]\n",
    "    print(model_name)\n",
    "    full_model_name = '{}.tar.gz'.format(model_name)\n",
    "    predict_one_house_value(gen_random_house()[1:], full_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically deploy another model\n",
    "\n",
    "여기서 신규 모델의 동적 로딩의 힘을 볼 수 있습니다. 이전에 모델을 배포할 때 의도적으로 첫 번째 모델을 복사하지 않았습니다. 이제 추가 모델을 배포하고 다중 모델 엔드 포인트를 통해 즉시 모델을 호출할 수 있습니다. 이전 모델과 마찬가지로 엔드포인트가 모델을 다운로드하고 메모리에 로드하는 데 시간이 걸리므로 새 모델을 처음 호출하는 데 시간이 약간 더 걸린다는 점을 명심해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying NewYork_NY model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/NewYork_NY/xgb-NewYork-NY-2019-11-27-06-15-54-555/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/NewYork_NY.tar.gz...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DEMO_MME_XGBOOST/multi_model_artifacts/NewYork_NY.tar.gz'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add another model to the endpoint and exercise it\n",
    "deploy_artifacts_to_mme(training_jobs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the newly deployed model\n",
    "\n",
    "엔드포인트 업데이트 또는 재시작 없이 새로 배포된 모델들로 호출을 수행해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the models that the endpoint has at its disposal:\n",
      "2019-11-27 06:46:19      11279 Chicago_IL.tar.gz\n",
      "2019-11-27 06:46:19      11184 Houston_TX.tar.gz\n",
      "2019-11-27 06:46:19      10751 LosAngeles_CA.tar.gz\n",
      "2019-11-27 07:00:39      10631 NewYork_NY.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model NewYork_NY.tar.gz to predict price of this house: [1996, 2258, 2, 2.0, 0.72, 0]\n",
      "$304,361.25, took 1,042 ms\n",
      "\n",
      "Using model NewYork_NY.tar.gz to predict price of this house: [2002, 2545, 2, 2.0, 1.13, 0]\n",
      "$376,476.25, took 21 ms\n",
      "\n",
      "Using model NewYork_NY.tar.gz to predict price of this house: [1989, 3741, 3, 1.5, 1.27, 1]\n",
      "$502,773.16, took 15 ms\n",
      "\n",
      "Using model NewYork_NY.tar.gz to predict price of this house: [1982, 2364, 2, 1.5, 1.36, 0]\n",
      "$265,714.44, took 16 ms\n",
      "\n",
      "Using model NewYork_NY.tar.gz to predict price of this house: [2003, 3056, 3, 2.5, 1.35, 0]\n",
      "$466,382.69, took 14 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = LOCATIONS[0]\n",
    "full_model_name = '{}.tar.gz'.format(model_name)\n",
    "for i in range(5):\n",
    "    features = gen_random_house()\n",
    "    predict_one_house_value(gen_random_house()[1:], full_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a model\n",
    "\n",
    "모델을 업데이트하려면 위와 동일한 방법으로 새 모델로 추가하세요. 예를 들어,`NewYork_NY.tar.gz` 모델을 재학습하고 호출을 시작하려는 경우 업데이트된 모델 아티팩트를 S3 접두어(prefix) 뒤에 `NewYork_NY_v2.tar.gz`와 같은 새로운 이름으로 업로드한 다음 `NewYork_NY.tar.gz` 대신`NewYork_NY_v2.tar.gz`를 호출하도록 `TargetModel` 필드를 변경하세요. 모델의 이전 버전이 여전히 컨테이너 또는 엔드포인트 인스턴스의 스토리지 볼륨에 로드될 수 있으므로 Amazon S3에서 모델 아티팩트를 덮어 쓰지 않으려고 합니다. 그러면 새 모델 호출 시 이전 버전의 모델을 호출할 수 있습니다.\n",
    "\n",
    "또는, 엔드포인트를 중지하고 새로운 모델 셋을 재배포할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "더 이상 사용하지 않는 엔드포인트에 대한 요금이 청구되지 않도록 리소스를 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'da8bc294-61f9-443d-b2d4-fd29d0173509',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'da8bc294-61f9-443d-b2d4-fd29d0173509',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 27 Nov 2019 07:52:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shut down the endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '79e06a1b-25cf-4651-8230-f9937ddcd64e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '79e06a1b-25cf-4651-8230-f9937ddcd64e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 27 Nov 2019 07:52:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the endpoint config\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f2953525-b07f-4508-ae1c-018c02b10ee5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f2953525-b07f-4508-ae1c-018c02b10ee5',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 27 Nov 2019 07:52:04 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete model too\n",
    "sm_client.delete_model(ModelName=multi_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
