{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using your own algorithm container\n",
    "\n",
    "*이 노트북은 [Amazon SageMaker Multi-Model Endpoints using your own algorithm container (영문 원본)](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb) 의 한국어 번역입니다.*\n",
    "\n",
    "고객들은 [Amazon SageMaker 멀티 모델 엔드포인트(multi-model endpoints)](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html)를 사용하여 최대 수천 개의 모델을 완벽하게 호스팅하는 엔드포인트를 생성할 수 있습니다. 이러한 엔드포인트는 공통 추론 컨테이너(common inference container)에서 제공할 수 있는 많은 모델 중 하나를 온디맨드(on demand)로 호출할 수 있어야 하고 자주 호출되지 않는 모델이 약간의 추가 대기 시간(latency) 허용이 가능한 사례들에 적합합니다. 지속적으로 낮은 추론 대기 시간이 필요한 애플리케이션의 경우 기존의 엔드포인트가 여전히 최선의 선택입니다.\n",
    "\n",
    "High level에서 Amazon SageMaker는 필요에 따라 멀티 모델 엔드포인트에 대한 모델 로딩 및 언로딩을 관리합니다. 특정 모델에 대한 호출 요청이 발생하면 Amazon SageMaker는 해당 모델에 할당된 인스턴스로 요청을 라우팅하고 S3에서 모델 아티팩트(model artifacts)를 해당 인스턴스로 다운로드한 다음 컨테이너의 메모리에 모델 로드를 시작합니다. 로딩이 완료되면 Amazon SageMaker는 요청된 호출을 수행하고 결과를 반환합니다. 모델이 선택된 인스턴스의 메모리에 이미 로드되어 있으면 다운로드 및 로딩 단계들을 건너 뛰고 즉시 호출이 수행됩니다.\n",
    "\n",
    "추론 컨테이너가 멀티 모델 엔드 포인트에서 여러 모델을 제공하려면 특정 모델의 로드(load), 나열(list), 가져오기(get), 언로드(unload) 및 호출(invoke)을 위한 [추가 API](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html)를 구현해야 합니다. 이 노트북은 이러한 API를 구현하는 자체 추론 컨테이너를 작성하는 방법을 보여줍니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. [Introduction to Multi Model Server (MMS)](#Introduction-to-Multi-Model-Server-(MMS))\n",
    "  1. [Handling Out Of Memory conditions](#Handling-Out-Of-Memory-conditions)\n",
    "  1. [SageMaker Inference Toolkit](#SageMaker-Inference-Toolkit)\n",
    "1. [Building and registering a container using MMS](#Building-and-registering-a-container-using-MMS)\n",
    "1. [Set up the environment](#Set-up-the-environment)\n",
    "1. [Upload model artifacts to S3](#Upload-model-artifacts-to-S3)\n",
    "1. [Create a multi-model endpoint](#Create-a-multi-model-endpoint)\n",
    "  1. [Import models into hosting](#Import-models-into-hosting)\n",
    "  1. [Create endpoint configuration](#Create-endpoint-configuration)\n",
    "  1. [Create endpoint](#Create-endpoint)\n",
    "1. [Invoke models](#Invoke-models)\n",
    "  1. [Add models to the endpoint](#Add-models-to-the-endpoint)\n",
    "  1. [Updating a model](#Updating-a-model)\n",
    "1. [(Optional) Delete the hosting resources](#(Optional)-Delete-the-hosting-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Multi Model Server (MMS)\n",
    "\n",
    "[Multi Model Server](https://github.com/awslabs/multi-model-server)는 머신 러닝 모델을 제공하기 위한 오픈 소스 프레임워크입니다. 단일 모델 내에서 여러 모델을 호스팅하고, 모델을 컨테이너에 동적으로 로드 및 언로드하고, 지정된 로드된 모델에 대한 추론을 수행하기 위해 다중 모델 엔드 포인트에 필요한 HTTP 프런트 엔드 및 모델 관리 기능을 제공합니다.\n",
    "\n",
    "MMS는 자체 알고리즘을 구현할 수 있는 플러그 가능한 커스텀 백엔드 핸들러(pluggable custom backend handler)를 지원합니다. 이 예제는 MXNet 모델의 로딩 및 추론을 지원하는 핸들러를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33mModelHandler defines an example model handler for load and inference requests for MXNet CPU models\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m namedtuple\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmx\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mModelHandler\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    A sample Model handler implementation.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.initialized = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.mx_model = \u001b[36mNone\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.shapes = \u001b[36mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_model_files_prefix\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, model_dir):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Get the model prefix name for the model artifacts (symbol and parameter file).\u001b[39;49;00m\r\n",
      "\u001b[33m        This assume model artifact directory contains a symbol file, parameter file, \u001b[39;49;00m\r\n",
      "\u001b[33m        model shapes file and a synset file defining the labels\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        :param model_dir: Path to the directory with model artifacts\u001b[39;49;00m\r\n",
      "\u001b[33m        :return: prefix string for model artifact files\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        sym_file_suffix = \u001b[33m\"\u001b[39;49;00m\u001b[33m-symbol.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        checkpoint_prefix_regex = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}/*{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_dir, sym_file_suffix) \u001b[37m# Ex output: /opt/ml/models/resnet-18/model/*-symbol.json\u001b[39;49;00m\r\n",
      "        checkpoint_prefix_filename = glob.glob(checkpoint_prefix_regex)[\u001b[34m0\u001b[39;49;00m] \u001b[37m# Ex output: /opt/ml/models/resnet-18/model/resnet18-symbol.json\u001b[39;49;00m\r\n",
      "        checkpoint_prefix = os.path.basename(checkpoint_prefix_filename).split(sym_file_suffix)[\u001b[34m0\u001b[39;49;00m] \u001b[37m# Ex output: resnet18\u001b[39;49;00m\r\n",
      "        logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mPrefix for the model artifacts: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(checkpoint_prefix))\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m checkpoint_prefix\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_input_data_shapes\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, model_dir, checkpoint_prefix):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Get the model input data shapes and return the list\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        :param model_dir: Path to the directory with model artifacts\u001b[39;49;00m\r\n",
      "\u001b[33m        :param checkpoint_prefix: Model files prefix name\u001b[39;49;00m\r\n",
      "\u001b[33m        :return: prefix string for model artifact files\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        shapes_file_path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33m{}-{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(checkpoint_prefix, \u001b[33m\"\u001b[39;49;00m\u001b[33mshapes.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.isfile(shapes_file_path):\r\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMissing {} file.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(shapes_file_path))\r\n",
      "\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(shapes_file_path) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.shapes = json.load(f)\r\n",
      "\r\n",
      "        data_shapes = []\r\n",
      "\r\n",
      "        \u001b[34mfor\u001b[39;49;00m input_data \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.shapes:\r\n",
      "            data_name = input_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "            data_shape = input_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "            data_shapes.append((data_name, \u001b[36mtuple\u001b[39;49;00m(data_shape)))\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m data_shapes\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minitialize\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, context):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Initialize model. This will be called during model loading time\u001b[39;49;00m\r\n",
      "\u001b[33m        :param context: Initial context contains model server system properties.\u001b[39;49;00m\r\n",
      "\u001b[33m        :return:\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.initialized = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "        properties = context.system_properties\r\n",
      "        \u001b[37m# Contains the url parameter passed to the load request\u001b[39;49;00m\r\n",
      "        model_dir = properties.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \r\n",
      "        gpu_id = properties.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mgpu_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "        checkpoint_prefix = \u001b[36mself\u001b[39;49;00m.get_model_files_prefix(model_dir)\r\n",
      "\r\n",
      "        \u001b[37m# Read the model input data shapes\u001b[39;49;00m\r\n",
      "        data_shapes = \u001b[36mself\u001b[39;49;00m.get_input_data_shapes(model_dir, checkpoint_prefix)\r\n",
      "         \r\n",
      "        \u001b[37m# Load MXNet model\u001b[39;49;00m\r\n",
      "        \u001b[34mtry\u001b[39;49;00m:\r\n",
      "            ctx = mx.cpu() \u001b[37m# Set the context on CPU\u001b[39;49;00m\r\n",
      "            sym, arg_params, aux_params = mx.model.load_checkpoint(checkpoint_prefix, \u001b[34m0\u001b[39;49;00m)  \u001b[37m# epoch set to 0\u001b[39;49;00m\r\n",
      "            \u001b[36mself\u001b[39;49;00m.mx_model = mx.mod.Module(symbol=sym, context=ctx, label_names=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "            \u001b[36mself\u001b[39;49;00m.mx_model.bind(for_training=\u001b[36mFalse\u001b[39;49;00m, data_shapes=data_shapes, \r\n",
      "                   label_shapes=\u001b[36mself\u001b[39;49;00m.mx_model._label_shapes)\r\n",
      "            \u001b[36mself\u001b[39;49;00m.mx_model.set_params(arg_params, aux_params, allow_missing=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "            \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msynset.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "                \u001b[36mself\u001b[39;49;00m.labels = [l.rstrip() \u001b[34mfor\u001b[39;49;00m l \u001b[35min\u001b[39;49;00m f]\r\n",
      "        \u001b[34mexcept\u001b[39;49;00m (mx.base.MXNetError, \u001b[36mRuntimeError\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m memerr:\r\n",
      "            \u001b[34mif\u001b[39;49;00m re.search(\u001b[33m'\u001b[39;49;00m\u001b[33mFailed to allocate (.*) Memory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m(memerr), re.IGNORECASE):\r\n",
      "                logging.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mMemory allocation exception: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(memerr))\r\n",
      "                \u001b[34mraise\u001b[39;49;00m \u001b[36mMemoryError\u001b[39;49;00m\r\n",
      "            \u001b[34mraise\u001b[39;49;00m           \r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpreprocess\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, request):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Transform raw input into model input data.\u001b[39;49;00m\r\n",
      "\u001b[33m        :param request: list of raw requests\u001b[39;49;00m\r\n",
      "\u001b[33m        :return: list of preprocessed model input data\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[37m# Take the input data and pre-process it make it inference ready\u001b[39;49;00m\r\n",
      "\r\n",
      "        img_list = []\r\n",
      "        \u001b[34mfor\u001b[39;49;00m idx, data \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(request):\r\n",
      "            \u001b[37m# Read the bytearray of the image from the input\u001b[39;49;00m\r\n",
      "            img_arr = data.get(\u001b[33m'\u001b[39;49;00m\u001b[33mbody\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)  \r\n",
      "\r\n",
      "            \u001b[37m# Input image is in bytearray, convert it to MXNet NDArray\u001b[39;49;00m\r\n",
      "            img = mx.img.imdecode(img_arr) \r\n",
      "            \u001b[34mif\u001b[39;49;00m img \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
      "                \u001b[34mreturn\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "            \u001b[37m# convert into format (batch, RGB, width, height)\u001b[39;49;00m\r\n",
      "            img = mx.image.imresize(img, \u001b[34m224\u001b[39;49;00m, \u001b[34m224\u001b[39;49;00m) \u001b[37m# resize\u001b[39;49;00m\r\n",
      "            img = img.transpose((\u001b[34m2\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)) \u001b[37m# Channel first\u001b[39;49;00m\r\n",
      "            img = img.expand_dims(axis=\u001b[34m0\u001b[39;49;00m) \u001b[37m# batchify\u001b[39;49;00m\r\n",
      "            img_list.append(img)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m img_list\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minference\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, model_input):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Internal inference methods\u001b[39;49;00m\r\n",
      "\u001b[33m        :param model_input: transformed model input data list\u001b[39;49;00m\r\n",
      "\u001b[33m        :return: list of inference output in NDArray\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[37m# Do some inference call to engine here and return output\u001b[39;49;00m\r\n",
      "        Batch = namedtuple(\u001b[33m'\u001b[39;49;00m\u001b[33mBatch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        \u001b[36mself\u001b[39;49;00m.mx_model.forward(Batch(model_input))\r\n",
      "        prob = \u001b[36mself\u001b[39;49;00m.mx_model.get_outputs()[\u001b[34m0\u001b[39;49;00m].asnumpy()\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m prob\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpostprocess\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, inference_output):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Return predict result in as list.\u001b[39;49;00m\r\n",
      "\u001b[33m        :param inference_output: list of inference output\u001b[39;49;00m\r\n",
      "\u001b[33m        :return: list of predict results\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[37m# Take output from network and post-process to desired format\u001b[39;49;00m\r\n",
      "        prob = np.squeeze(inference_output)\r\n",
      "        a = np.argsort(prob)[::-\u001b[34m1\u001b[39;49;00m]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m [[\u001b[33m'\u001b[39;49;00m\u001b[33mprobability=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m, class=\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %(prob[i], \u001b[36mself\u001b[39;49;00m.labels[i]) \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m a[\u001b[34m0\u001b[39;49;00m:\u001b[34m5\u001b[39;49;00m]]]\r\n",
      "        \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mhandle\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, data, context):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Call preprocess, inference and post-process functions\u001b[39;49;00m\r\n",
      "\u001b[33m        :param data: input data\u001b[39;49;00m\r\n",
      "\u001b[33m        :param context: mms context\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \r\n",
      "        model_input = \u001b[36mself\u001b[39;49;00m.preprocess(data)\r\n",
      "        model_out = \u001b[36mself\u001b[39;49;00m.inference(model_input)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.postprocess(model_out)\r\n",
      "\r\n",
      "_service = ModelHandler()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mhandle\u001b[39;49;00m(data, context):\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m _service.initialized:\r\n",
      "        _service.initialize(context)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m data \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m _service.handle(data, context)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/model_handler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`handle(data, context)` 및 `initialize(self, context)` 메소드가 중요합니다.\n",
    "\n",
    "모델이 메모리에 로드될 때 `initialize` 메소드가 호출됩니다. 이 예제에서는 `model_dir`의 모델 아티팩트를 MXNet에 로드합니다.<br>\n",
    "모델을 호출할 때 `handle` 메소드가 호출됩니다. 이 예제에서는 입력 페이로드의 유효성을 검사한 다음, 입력값을 MXNet에 전달하여 출력값을 반환합니다.<br>\n",
    "이 핸들러 클래스는 컨테이너에 로드된 모든 모델에 대해 인스턴스화되므로, 핸들러의 상태가 모델 간에 공유되지 않습니다.\n",
    "\n",
    "### Handling Out Of Memory conditions\n",
    "\n",
    "메모리 부족으로 MXNet이 모델을 로드하지 못하면 `MemoryError`가 발생합니다. 메모리 부족이나 다른 리소스 제약으로 인해 모델을 로드할 수 없는 경우에는 항상 `MemoryError`를 발생시켜야 합니다. MMS는 `MemoryError`를 해석하고 507 HTTP 상태 코드를 SageMaker에 반환합니다. 여기서 SageMaker는 사용되지 않은 모델 언로딩을 시작(initiate)하여 리소스를 회수(reclaim)하여 요청된 모델을 로드할 수 있습니다.\n",
    "\n",
    "### SageMaker Inference Toolkit\n",
    "MMS 시작 시 프론트엔드 서버에 대한 [다양한 설정들](https://github.com/awslabs/multi-model-server/blob/master/docker/advanced_settings.md#description-of-config-file-settings)을 지원합니다.\n",
    "\n",
    "[SageMaker Inference Toolkit](https://github.com/aws/sagemaker-inference-toolkit)은 SageMaker 멀티 모델 엔드포인트와 호환되는 방식으로 MMS를 부트스트랩하는 라이브러리로 중요한 성능 파라메터들을 조정할 수 있습니다. 모델 당 worker 수와 같은 이 예제의 추론 컨테이너는 추론 툴킷(Inference Toolkit)을 사용하여 MMS를 시작합니다. MMS 시작에 관한 스크립트는 __`container/dockerd-entrypoint.py`__ 파일에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and registering a container using MMS\n",
    "\n",
    "아래 쉘 스크립트는 MMS를 프론트엔드(SageMaker Inference Toolkit을 통해 설정)로 사용하는 Docker 이미지(image)와 위에서 백엔드 핸들러로 확인한 `container/model_handler.py` 를 빌드합니다. 그런 다음, 이미지를 계정의 ECR 저장소에 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "sha256:060139c29046dbaebeb7287af528b8267a243e121f8805f4856075cd34310051\n",
      "The push refers to repository [143656149352.dkr.ecr.ap-northeast-2.amazonaws.com/demo-sagemaker-multimodel]\n",
      "7d956ab809ad: Preparing\n",
      "6678f392964e: Preparing\n",
      "84176b0a0a11: Preparing\n",
      "10bca8733be2: Preparing\n",
      "aa9339a0976b: Preparing\n",
      "c227ec35d60e: Preparing\n",
      "ddf4568454c0: Preparing\n",
      "cd19ebef0b86: Preparing\n",
      "aa7f8c8d5f39: Preparing\n",
      "48817fbd6c92: Preparing\n",
      "1b039d138968: Preparing\n",
      "7082d7d696f8: Preparing\n",
      "c227ec35d60e: Waiting\n",
      "ddf4568454c0: Waiting\n",
      "cd19ebef0b86: Waiting\n",
      "aa7f8c8d5f39: Waiting\n",
      "48817fbd6c92: Waiting\n",
      "1b039d138968: Waiting\n",
      "7082d7d696f8: Waiting\n",
      "84176b0a0a11: Pushed\n",
      "10bca8733be2: Pushed\n",
      "7d956ab809ad: Pushed\n",
      "6678f392964e: Pushed\n",
      "aa7f8c8d5f39: Pushed\n",
      "c227ec35d60e: Pushed\n",
      "ddf4568454c0: Pushed\n",
      "48817fbd6c92: Pushed\n",
      "1b039d138968: Pushed\n",
      "7082d7d696f8: Pushed\n",
      "aa9339a0976b: Pushed\n",
      "cd19ebef0b86: Pushed\n",
      "latest: digest: sha256:7e4754b3d6b9c60cf25b412d0768503d7f1889e2470bac0b1159728bc2d3c3bc size: 2820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=demo-sagemaker-multimodel\n",
    "\n",
    "cd container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -q -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "멀티 모델 엔드포인트에서 호출할 수 있는 모델 아티팩트가 있는 S3 버킷 및 접두부(prefix)를 정의합니다.\n",
    "\n",
    "또한 SageMaker가 위에서 생성한 모델 아티팩트 및 ECR 이미지에 액세스할 수 있도록 IAM 역할을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket = 'sagemaker-{}-{}'.format(region, account_id)\n",
    "prefix = 'demo-multimodel-endpoint'\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model artifacts to S3\n",
    "이 예시에서 ImageNet datset에 대해 사전 학습된 ResNet 18 및 ResNet 152 모델을 사용합니다. 먼저 MXNet model zoo에서 모델을 다운로드한 다음 모델을 S3에 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "model_path = 'http://data.mxnet.io/models/imagenet/'\n",
    "\n",
    "mx.test_utils.download(model_path+'resnet/18-layers/resnet-18-0000.params', None, 'data/resnet_18')\n",
    "mx.test_utils.download(model_path+'resnet/18-layers/resnet-18-symbol.json', None, 'data/resnet_18')\n",
    "mx.test_utils.download(model_path+'synset.txt', None, 'data/resnet_18')\n",
    "\n",
    "with open('data/resnet_18/resnet-18-shapes.json', 'w') as file:\n",
    "    file.write('[{\"shape\": [1, 3, 224, 224], \"name\": \"data\"}]')\n",
    "    \n",
    "with tarfile.open('data/resnet_18.tar.gz', 'w:gz') as tar:\n",
    "    tar.add('data/resnet_18', arcname='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.test_utils.download(model_path+'resnet/152-layers/resnet-152-0000.params', None, 'data/resnet_152')\n",
    "mx.test_utils.download(model_path+'resnet/152-layers/resnet-152-symbol.json', None, 'data/resnet_152')\n",
    "mx.test_utils.download(model_path+'synset.txt', None, 'data/resnet_152')\n",
    "\n",
    "with open('data/resnet_152/resnet-152-shapes.json', 'w') as file:\n",
    "    file.write('[{\"shape\": [1, 3, 224, 224], \"name\": \"data\"}]')\n",
    "    \n",
    "with tarfile.open('data/resnet_152.tar.gz', 'w:gz') as tar:\n",
    "    tar.add('data/resnet_152', arcname='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://data.mxnet.io/models/imagenet/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import ClientError\n",
    "import os\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    s3.meta.client.head_bucket(Bucket=bucket)\n",
    "except ClientError:\n",
    "    s3.create_bucket(Bucket=bucket,\n",
    "                     CreateBucketConfiguration={\n",
    "                         'LocationConstraint': region\n",
    "                     })\n",
    "\n",
    "models = {'resnet_18.tar.gz', 'resnet_152.tar.gz'}\n",
    "\n",
    "for model in models:\n",
    "    key = os.path.join(prefix, model)\n",
    "    with open('data/'+model, 'rb') as file_obj:\n",
    "        s3.Bucket(bucket).Object(key).upload_fileobj(file_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a multi-model endpoint\n",
    "### Import models into hosting\n",
    "멀티 모델 엔드포인트에 대한 모델 엔티티를 작성할 때 컨테이너의 `ModelDataUrl`은 엔드포인트에서 호출 할 수 있는 모델 아티팩트가 있는 S3 접두부(prefix)입니다. 나머지 S3 경로는 모델을 호출할 때 지정됩니다.\n",
    "\n",
    "컨테이너 모드(`Mode`)는 컨테이너가 여러 모델들을 호스팅함을 나타내기 위해 `MultiModel`로 지정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: DEMO-MultiModelModel2019-11-27-08-29-01\n",
      "Model data Url: https://s3-ap-northeast-2.amazonaws.com/sagemaker-ap-northeast-2-143656149352/demo-multimodel-endpoint/\n",
      "Container image: 143656149352.dkr.ecr.ap-northeast-2.amazonaws.com/demo-sagemaker-multimodel:latest\n",
      "Model Arn: arn:aws:sagemaker:ap-northeast-2:143656149352:model/demo-multimodelmodel2019-11-27-08-29-01\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = 'DEMO-MultiModelModel' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = 'https://s3-{}.amazonaws.com/{}/{}/'.format(region, bucket, prefix)\n",
    "container = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, 'demo-sagemaker-multimodel')\n",
    "\n",
    "print('Model name: ' + model_name)\n",
    "print('Model data Url: ' + model_url)\n",
    "print('Container image: ' + container)\n",
    "\n",
    "container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_url,\n",
    "    'Mode': 'MultiModel'\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container])\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "엔드포인트 설정 생성은 단일 모델 엔드포인트와 동일한 방식으로 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: DEMO-MultiModelEndpointConfig-2019-11-27-08-29-12\n",
      "Endpoint config Arn: arn:aws:sagemaker:ap-northeast-2:143656149352:endpoint-config/demo-multimodelendpointconfig-2019-11-27-08-29-12\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = 'DEMO-MultiModelEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 2,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "마찬가지로, 엔드포인트 생성은 단일 모델 엔드포인트와 동일한 방식으로 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: DEMO-MultiModelEndpoint-2019-11-27-08-29-16\n",
      "Endpoint Arn: arn:aws:sagemaker:ap-northeast-2:143656149352:endpoint/demo-multimodelendpoint-2019-11-27-08-29-16\n",
      "Endpoint Status: Creating\n",
      "Waiting for DEMO-MultiModelEndpoint-2019-11-27-08-29-16 endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'DEMO-MultiModelEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke models\n",
    "이제 이전에 S3에 업로드한 모델을 호출해 보겠습니다. SageMaker가 모델 아티팩트를 S3에서 인스턴스로 다운로드하여 컨테이너에 로드하므로 모델의 첫번째 호출이 느려질 수 있습니다.\n",
    "\n",
    "먼저 고양이 이미지를 페이로드로 다운로드하여 모델을 호출한 다음 `InvokeEndpoint`를 호출하여 ResNet 18 모델을 호출합니다. `TargetModel` 필드는 모델 작성시 `ModelDataUrl`에 지정된 S3 접두부와 연결되어 S3에서 모델의 위치를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = mx.test_utils.download('https://github.com/dmlc/web-data/blob/master/mxnet/doc/tutorials/python/predict_image/cat.jpg?raw=true', 'cat.jpg')\n",
    "\n",
    "with open(fname, 'rb') as f:\n",
    "    payload = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability=0.244390, class=n02119022 red fox, Vulpes vulpes\n",
      "probability=0.170341, class=n02119789 kit fox, Vulpes macrotis\n",
      "probability=0.145019, class=n02113023 Pembroke, Pembroke Welsh corgi\n",
      "probability=0.059833, class=n02356798 fox squirrel, eastern fox squirrel, Sciurus niger\n",
      "probability=0.051555, class=n02123159 tiger cat\n",
      "CPU times: user 13.5 ms, sys: 0 ns, total: 13.5 ms\n",
      "Wall time: 2.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-image',\n",
    "    TargetModel='resnet_18.tar.gz', # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=payload)\n",
    "\n",
    "print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 ResNet 18 모델을 두번째로 호출하면, 이미 인스턴스에 다운로드되어 컨테이너에 로드되므로 추론이 더 빠르게 수행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability=0.244390, class=n02119022 red fox, Vulpes vulpes\n",
      "probability=0.170341, class=n02119789 kit fox, Vulpes macrotis\n",
      "probability=0.145019, class=n02113023 Pembroke, Pembroke Welsh corgi\n",
      "probability=0.059833, class=n02356798 fox squirrel, eastern fox squirrel, Sciurus niger\n",
      "probability=0.051555, class=n02123159 tiger cat\n",
      "CPU times: user 6.03 ms, sys: 0 ns, total: 6.03 ms\n",
      "Wall time: 156 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-image',\n",
    "    TargetModel='resnet_18.tar.gz',\n",
    "    Body=payload)\n",
    "\n",
    "print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke another model\n",
    "멀티 모델 엔드 포인트의 성능을 발휘하기 위해 다른 모델(`resnet_152.tar.gz`)을 `TargetModel`로 지정하고 동일한 엔드포인트를 사용하여 추론을 수행 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability=0.386026, class=n02119022 red fox, Vulpes vulpes\n",
      "probability=0.300927, class=n02119789 kit fox, Vulpes macrotis\n",
      "probability=0.029575, class=n02123045 tabby, tabby cat\n",
      "probability=0.026005, class=n02123159 tiger cat\n",
      "probability=0.023201, class=n02113023 Pembroke, Pembroke Welsh corgi\n",
      "CPU times: user 15.5 ms, sys: 80 µs, total: 15.5 ms\n",
      "Wall time: 7.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-image',\n",
    "    TargetModel='resnet_152.tar.gz',\n",
    "    Body=payload)\n",
    "\n",
    "print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add models to the endpoint\n",
    "엔드포인트를 업데이트하지 않고도 엔드포인트에 더 많은 모델을 추가할 수 있습니다. 아래에는 `squeezenet_v1.0`이라는 세번째 모델이 추가되었습니다. 엔드포인트 뒤에 여러 모델들을 호스팅하는 방법을 보여주기 위해, S3에서 이름들을 약간 다르게 하여 10번 복제합니다. 보다 현실적인 시나리오에서는 10개의 새로운 모델이 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.test_utils.download(model_path+'squeezenet/squeezenet_v1.0-0000.params', None, 'data/squeezenet_v1.0')\n",
    "mx.test_utils.download(model_path+'squeezenet/squeezenet_v1.0-symbol.json', None, 'data/squeezenet_v1.0')\n",
    "mx.test_utils.download(model_path+'synset.txt', None, 'data/squeezenet_v1.0')\n",
    "\n",
    "with open('data/squeezenet_v1.0/squeezenet_v1.0-shapes.json', 'w') as file:\n",
    "    file.write('[{\"shape\": [1, 3, 224, 224], \"name\": \"data\"}]')\n",
    "    \n",
    "with tarfile.open('data/squeezenet_v1.0.tar.gz', 'w:gz') as tar:\n",
    "    tar.add('data/squeezenet_v1.0', arcname='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models: 12\n",
      "Models: {'demo-subfolder/squeezenet_v1.0_0.tar.gz', 'demo-subfolder/squeezenet_v1.0_2.tar.gz', 'resnet_18.tar.gz', 'demo-subfolder/squeezenet_v1.0_1.tar.gz', 'demo-subfolder/squeezenet_v1.0_6.tar.gz', 'demo-subfolder/squeezenet_v1.0_5.tar.gz', 'demo-subfolder/squeezenet_v1.0_8.tar.gz', 'resnet_152.tar.gz', 'demo-subfolder/squeezenet_v1.0_9.tar.gz', 'demo-subfolder/squeezenet_v1.0_4.tar.gz', 'demo-subfolder/squeezenet_v1.0_7.tar.gz', 'demo-subfolder/squeezenet_v1.0_3.tar.gz'}\n"
     ]
    }
   ],
   "source": [
    "file = 'data/squeezenet_v1.0.tar.gz'\n",
    "\n",
    "for x in range(0, 10):\n",
    "    s3_file_name = 'demo-subfolder/squeezenet_v1.0_{}.tar.gz'.format(x)\n",
    "    key = os.path.join(prefix, s3_file_name)\n",
    "    with open(file, 'rb') as file_obj:\n",
    "        s3.Bucket(bucket).Object(key).upload_fileobj(file_obj)\n",
    "    models.add(s3_file_name)\n",
    "\n",
    "print('Number of models: {}'.format(len(models)))\n",
    "print('Models: {}'.format(models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SqueezeNet 모델을 S3에 업로드한 후 각 호출에 대해 S3 접두부(prefix) 뒤에 있는 12개 모델 중 하나를 임의로 선택하고, 각 호출 응답(invoke response)에서 가장 높은 확률을 출력하는 레이블 개수를 출력하기 위해 엔드포인트를 100번 호출해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'demo-subfolder/squeezenet_v1.0_0.tar.gz',\n",
       " 'demo-subfolder/squeezenet_v1.0_1.tar.gz',\n",
       " 'demo-subfolder/squeezenet_v1.0_2.tar.gz',\n",
       " 'demo-subfolder/squeezenet_v1.0_3.tar.gz',\n",
       " 'demo-subfolder/squeezenet_v1.0_4.tar.gz',\n",
       " 'demo-subfolder/squeezenet_v1.0_5.tar.gz',\n",
       " 'demo-subfolder/squeezenet_v1.0_6.tar.gz',\n",
       " 'demo-subfolder/squeezenet_v1.0_7.tar.gz',\n",
       " 'demo-subfolder/squeezenet_v1.0_8.tar.gz',\n",
       " 'demo-subfolder/squeezenet_v1.0_9.tar.gz',\n",
       " 'resnet_152.tar.gz',\n",
       " 'resnet_18.tar.gz'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('probability=0.386026, class=n02119022 red fox, Vulpes vulpes', 5)\n",
      "('probability=0.294885, class=n02326432 hare', 86)\n",
      "('probability=0.244390, class=n02119022 red fox, Vulpes vulpes', 9)\n",
      "CPU times: user 380 ms, sys: 23.9 ms, total: 404 ms\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "results = defaultdict(int)\n",
    "\n",
    "for x in range(0, 100):\n",
    "    target_model = random.choice(tuple(models))\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/x-image',\n",
    "        TargetModel=target_model,\n",
    "        Body=payload)\n",
    "\n",
    "    results[json.loads(response['Body'].read())[0]] += 1\n",
    "    \n",
    "print(*results.items(), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a model\n",
    "모델을 업데이트하려면 위와 동일한 방법으로 새 모델로 추가하세요. 예를 들어, `resnet_18.tar.gz` 모델을 재학습했으며 호출을 시작하려는 경우, `resnet_18_v2.tar.gz`와 같은 새로운 이름으로 S3 접두사 뒤에 업데이트된 모델 아티팩트를 업로드한 다음 `TargetModel` 필드를 변경합니다. 그러면, `resnet_18.tar.gz` 대신 `resnet_18_v2.tar.gz`를 호출합니다. 모델의 이전 버전이 여전히 컨테이너 또는 엔드포인트 인스턴스의 스토리지 볼륨에 로드 될 수 있으므로 Amazon S3에서 모델 아티팩트를 덮어 쓰지 않으려고 합니다. 새 모델을 호출하면 이전 버전의 모델을 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Delete the hosting resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
